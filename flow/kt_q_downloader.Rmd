---
title: "larimer_q_downloader"
author: "Katie Willi"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(jsonlite)
library(httr)
library(arrow)
source("package_loader.R")
```

###### Alternative workflow for `larimer_q_downloader`

Sites of interest:
```{r}
clp_stations <-  c("11531", "11530", "11525", "11514", "11004", "11515",
                   "11518", "11517", "11516", "6770", "11083", "11082",
                   "11009","7130","10408", "11021")

bigt_stations <- c("11567", "11566", "11564", "11563", "11562", "11561",
                   "11560", "11559", "11558", "95100", "95000", "3640",
                   "3610", "3570", "3520")
```

Getting meta-data
```{r}
meta_urler <- function(stations){
  
  # sub-test it:
  # stations <- "11567"
  
  url <- paste0("https://larimerco-ns5.trilynx-novastar.systems/novastar/data/api/v1/",
                "stationSummaries?forOperatorStationDashboard=true&stationNumId=", stations, 
                "&periodStart=2022-01-25T13:59:00-07:00&periodEnd=", Sys.Date(),
                "T13:59:00-07:00")
  
  request <- httr::GET(url = url)
  
  total_list <- httr::content(request)
  
  basics <- total_list[["stationSummaries"]][[1]] %>% 
    rbind() %>% 
    as_tibble() %>%
    dplyr::select(id, numId, name, elevation, lat = latitude, long = longitude)
  
  station_meta <- as.data.frame(do.call(rbind, 
                                        total_list[["stationSummaries"]][[1]][["dataTypes"]] )) %>%
    rename(dataType = name) %>%
    cbind(basics)
  
  Sys.sleep(1)  
  
  return(station_meta)
  
}

clp_meta <- clp_stations %>%
  map(~ meta_urler(stations = .)) %>%
  dplyr::bind_rows() %>%
  mutate(WS = "CLP")

bigt_meta <- bigt_stations %>%
  map(~ meta_urler(stations = .)) %>%
  dplyr::bind_rows() %>%
  mutate(WS = "BigT")

final_meta <- bind_rows(clp_meta, bigt_meta) %>% 
  as.data.frame() %>%
  dplyr::select(WS, description = name, id, numId, dataType, elevation, lat, long) %>%
  # i think the list structure is making things funky... so explicitly calling 
  # out what each column type is here:
  mutate(WS = as.character(WS),
         description = as.character(description),
         id = as.numeric(id),
         numId = as.numeric(numId),
         dataType = as.character(dataType),
         elevation = as.numeric(elevation),
         lat = as.numeric(lat),
         long = as.numeric(long), 
        site_desc = case_when(grepl("Fort Collins - ", description) 
                                                                  ~ gsub(replacement = "", "Fort Collins - ", x = description),
                                grepl("Greeley - ", description) 
                                                                  ~ gsub(replacement = "", "Greeley - ", x = description),
                                grepl("Loveland - ", description) 
                                                                  ~ gsub(replacement = "", "Loveland - ", x = description), 
                                grepl("Larimer - ", description) 
                                                                  ~ gsub(replacement = "", "Larimer - ", x = description), 
                             grepl("Windsor - ", description) 
                                                                  ~ gsub(replacement = "", "Windsor - ", x = description),
                             description == "Poudre River at Hewlett Gulch Bridge" ~ "Poudre River at Hewlett Gulch Bridge")) 
```

Discharge data:
```{r}
q_sites <- final_meta %>%
  filter(grepl("Discharge", dataType)) %>%
  distinct(numId) %>%
  unlist() %>%
  unname()

q_downloader <- function(q_sites){
  
  # sub-test it:
  #q_sites <- "11518"
  
  url <- paste0("https://larimerco-ns5.trilynx-novastar.systems/novastar/data/api/v1/",
                "stationSummaries?forOperatorStationDashboard=true&stationNumId=", q_sites,
                "&periodStart=2015-01-25T13:59:00-07:00&periodEnd=", Sys.Date(),
                "T13:59:00-07:00")
  
  request <- httr::GET(url = url)
  
  total_list <- httr::content(request)
  
  discharge_rows <- as_tibble(do.call(rbind, 
                                      total_list[["stationSummaries"]][[1]][["ts"]])) %>%
    rowid_to_column() %>%
    filter(grepl("Discharge",dataType)) %>%
    pull(rowid)
  
  q_unlister <- function(discharge_rows){
    
    q_meta <- total_list[["stationSummaries"]][[1]][["ts"]][[as.numeric(discharge_rows)]] %>%
      rbind() %>%
      as.data.frame() %>%
      dplyr::select(-c(data, properties))
    
    unlisted_q <- total_list[["stationSummaries"]][[1]][["ts"]][[as.numeric(discharge_rows)]][["data"]] %>%
      bind_rows() %>%
      mutate(locId = as.character(q_meta$locId)) %>%
      #left_join(q_meta, by = "locId") %>%
      as.data.frame()
  
    return(unlisted_q)
    
}
  
  flow <- discharge_rows %>%
    map(~ q_unlister(discharge_rows = .)) %>%
    dplyr::bind_rows()
  
  return(flow)
  
}

q_for_all <- q_sites %>%
  map(~ q_downloader(q_sites = .)) %>%
  list_rbind()



meta_for_join <- final_meta%>%
  filter(dataType == "DischargeRiver")%>%
  mutate(locId = as.character(numId))%>%
  distinct(locId, .keep_all = TRUE)

q_for_all_wmeta <- q_for_all%>%
  left_join(meta_for_join, by = "locId")%>%
  mutate(datetime = lubridate::ymd_hms(dt),
         q_cfs = as.numeric(v)) 
  
```

## Check the Q

I'd recommend going to https://larimerco-ns5.trilynx-novastar.systems/novastar/operator/#/stationDashboard/100
to double check and make sure the data looks similar to the sites you pulled

```{r}
test_df_q <- q_for_all_wmeta %>%
  filter( locId == "6770")%>%
  select(datetime, q_cfs, locId) %>%
  as_tibble()




#werks
ggplot(data = test_df_q) +
  geom_line(aes(x = unlist(datetime), y = q_cfs)) +
  theme_bw() 
facet_wrap(~ locId)



```

# Exporting Q to arrow
```{r}

write_feather(q_for_all_wmeta, sink = "data/Q_modeling/larimer_co_2015-01-25_2023-01-30")

```

# Looking for all sites

```{r}

look_test <- GET(url = "https://larimerco-ns5.trilynx-novastar.systems/novastar/data/api/v1/stationSummaries?forOperatorStationSummary=true")
look_content <- httr::content(look_test)


look_final <- tibble()
no_station_data <- tibble()

## Functionalize the loop below and return a df named look_final with all the station data
## and a df named no_station_data with all the stations that don't have any data



  for(i in 1:length(look_content[["stationSummaries"]])){
  
    column_names <- c("id", "numId", "name", "elevation", "lat", "long" )
    
 basics <- look_content[["stationSummaries"]][[i]] %>% 
    rbind() %>% 
    as_tibble() %>%
    dplyr::select(id, numId, name, elevation, lat = latitude, long = longitude)%>%
    ## make all data characters rather than lists or factors
      mutate_at(vars(id, numId, name, elevation, lat, long), as.character)

## write code to skip station that doesn't have any dataType data 
  if(length(look_content[["stationSummaries"]][[i]][["dataTypes"]]) == 0){
    no_station_data <- bind_rows(no_station_data, basics)
    next
  }

  station_meta <- as.data.frame(do.call(rbind, 
                                        look_content[["stationSummaries"]][[i]][["dataTypes"]] ))%>%
    mutate(dataType = as.character(name)) %>%
    distinct(dataType)%>%
    rowid_to_column()%>%
    mutate(breaker = i)%>%
    cbind(basics)%>%
    select(dataType, numId, id, name, lat, long, rowid, breaker)%>%
    pivot_wider(., names_from = dataType, values_from = rowid )%>%
    as_tibble()
  
  look_final <- bind_rows(look_final, station_meta)
  }

## Use  sf to transform at the locations of the sites in look final and no_station_data
## remove NAs from lat and long columns from look final and no_station_data 



look_final <- look_final%>%
  filter(!is.na(lat) & !is.na(long)& lat != "NULL")
no_station_data <- no_station_data%>%
  filter(!is.na(lat)& !is.na(long)& lat != "NULL")

  

  ## create columns that classify sites by agency using the name column going off the list "Larimer", "Fort Collins", "Greeley" and "USGS"
  # Label other sites as "Other" in the name column
  look_final <- look_final%>%
    mutate(agency = case_when(
      grepl("Larimer -", name) ~ "Larimer",
      grepl("Fort Collins -", name) ~ "Fort Collins",
      grepl("Greeley -", name) ~ "Greeley",
      grepl("USGS", name) ~ "USGS",
      grepl("Loveland -", name) ~ "Loveland",
      grepl("Weld - ", name) ~ "Weld",
      grepl("Windsor - ", name) ~ "Windsor",
      TRUE ~ "Other"
    ))

    #remove the agency name from the name column
    look_final <- look_final%>%
      mutate(name = gsub("Larimer - ", "", name))%>%
      mutate(name = gsub("Fort Collins - ", "", name))%>%
      mutate(name = gsub("Greeley - ", "", name))%>%
      mutate(name = gsub("USGS ", "", name))%>%
      mutate(name = gsub("Loveland - ", "", name))%>%
      mutate(name = gsub("Weld - ", "", name))%>%
      mutate(name = gsub("Windsor - ", "", name))


## Count the number of sites by agency
look_final%>%
  group_by(agency)%>%
  count()%>%
  arrange(desc(n))

## Classify sites by data_available, if the site has data in the dischargeRiver and Precip columns, then it is classified as "both". If it has only dischargeRiver data, then it is classified as "discharge". If it has only Precipitation data, then it is classified as "precipitation". If it has neither, then it is classified as "none"
look_final <- look_final%>%
  mutate(data_available = case_when(
    !is.na(DischargeRiver) & !is.na(Precip) ~ "Precip and Q",
    !is.na(DischargeRiver) & is.na(Precip) ~ "Q",
    is.na(DischargeRiver) & !is.na(Precip) ~ "Precip",
    is.na(DischargeRiver) & is.na(Precip) ~ "none"
  ))

## use mapview to look at the sites in both dataframes

look_final_sf <- look_final%>%
  mutate(lat = as.numeric(lat),
         long = as.numeric(long))%>%
  st_as_sf(coords = c("long", "lat"), crs = 4326)

no_station_data_sf <- no_station_data%>%
mutate(lat = as.numeric(lat),
         long = as.numeric(long))%>%
  st_as_sf(coords = c("long", "lat"), crs = 4326)

  station_map <- mapview::mapview(look_final_sf, zcol = "data_available", alpha.regions = 0.5, map.types = "Esri.WorldTopoMap")


  station_map

## create a dataframe and csv that has the lat, long, site name, agency and data available for each site from look_final

look_final%>%
  filter(data_available == "none")%>%
  select(lat, long, name, agency, data_available)%>%
  write_csv("data/Q_modeling/station_data.csv")

```
  

```

