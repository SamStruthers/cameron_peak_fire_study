---
title: "Landsat Surface Temperature Post-processing"
format: html
editor: visual
---

## Purpose

This document handles post-processing of the surface temperature output from the 'LandsatC2Stacks.qmd' document to check for valid values, based on the methodology described in Herrick and Steele, et al. (Ecosphere, in press). This script removes scenes where the minimum temperature for the lake image is less than 0 deg C and where there are less thank 10 pixels contributing to the median surface temperature value. Additionally, it flags data that are outliers using the anomalize package.

```{r}
library(tidyverse)
library(ggthemes)
library(tibbletime)
library(anomalize)
```

## Pre-requisites

For this script to work as-written, you'll need to have downloaded the output SurfTemp files and add them to a folder called 'SurfTemp' within the directory this project is working from, or point to the directory they are found in within the code chunk below.

```{r}
dir = 'SurfTemp'
```

Now, we'll get the list file names and load them into the R environment as 'tempData'

```{r}
loadList = list.files(dir)

#iterate over the list of files
for(i in 1:length(loadList)){
  df = read.csv(file.path(dir, loadList[i]))
  if(i == 1){
    tempData = df
  } else {
  tempData = full_join(tempData, df)
  }
}
```

Now, point to the location file that you loaded into the 'LandsatC2Stacks.qmd' document and load in the file

```{r}
locsFile = 'upper_poudre_lakes.csv'

locs = read.csv(locsFile) %>% 
  mutate(comid = as.character(comid))
```

## Post Processing

First thing's first, let's grab the Landsat mission, path, row, date of acquisition, and comid from the 'system:index' column in the tempData file.

```{r}
tempData <- tempData %>% 
  mutate(LSMission = map_chr(system.index, function(s) rev(str_split(s, '_')[[1]])[4]),
         PR = map_chr(system.index, function(s) rev(str_split(s, '_')[[1]])[3]),
         date = map_chr(system.index, function(s) rev(str_split(s, '_')[[1]])[2]),
         comid = map_chr(system.index, function(s) rev(str_split(s, '_')[[1]])[1]))%>% 
  mutate(date = base::as.Date(date, format = '%Y%m%d')) 
```

Even with substantial QA/QC measures that are withiin the GEE script, the Landsat temperature data still needs to pass through some filters. For the temperature data, let's make sure there are more than 10 pixels contributing to the median temp value and remove any scenes where the minimum temperature is 0degC. (You'll notice this removes a lot of scenes, but most of these are scenes that had no data associated with them in the first place!)

```{r}
#filter out lake-scenes with 10 or fewer pixels contributing to the median
tempData_filtered <- tempData %>% 
  filter(pCount_SurfTemp >= 10)
# filter out all scenes where the minimum is below freezing (273.15K)
tempData_filtered <- tempData_filtered %>% 
  filter(min_SurfTempMin > 273.15)
```

Now, we can join the data with the location information and convert to degC.

```{r}
tempData_filtered <- left_join(tempData_filtered, locs) %>% 
  mutate(SurfTemp_degC = SurfTemp -273.15)
```

And now let's look at the data

```{r}
ggplot(tempData_filtered, aes(x = date, y = SurfTemp_degC)) +
  geom_point() +
  facet_grid(Name ~ .) +
  theme_bw()

tempData_filtered %>% 
  filter(date >= '2015-01-01') %>% 
  ggplot(., aes(x = date, y = SurfTemp_degC, color = sd_SurfTempSD)) +
  geom_point() +
  facet_grid(Name ~ .) +
  theme_bw()
```

These values look okay, but there are some wonky outliers that don't seem to be defined exclusively by a high standard deviation. Let's use the surrounding data to flag data that do not conform to the general trend of increasing and decreasing temperatures. To do this, we'll use the `anomalize` package, which handles timeseries data particularily well. We're going to do this one lake at a time, because each of the lakes' data need to be handled individually.

### Horsetooth Reservoir

```{r}
#grab one res and date and temp
HR <- tempData_filtered %>% 
  filter(Name == 'Horsetooth Reservoir') %>% 
  select(date, SurfTemp_degC, Name) %>% 
  arrange(date)
#convert to a tibble
HR <- as_tibble(HR)

#both trend and alpha probably need to be played around with per lake.
HR_anom <- HR %>% 
  time_decompose(SurfTemp_degC, merge = T, trend = '3 months') %>% 
  anomalize(remainder, alpha = 0.03) %>% 
  time_recompose()

HR_anom %>% 
  filter(date > as.Date('2010-01-01'))%>% 
  plot_anomalies(time_recomposed = T)
HR_anom %>% 
  plot_anomaly_decomposition()

HR <- full_join(HR, HR_anom) %>% 
  select(Name, date, SurfTemp_degC, anomaly)

```

### Milton Seaman Reservoir

```{r}
#grab one res and date and temp
MS <- tempData_filtered %>% 
  filter(Name == 'Milton Seaman Reservoir') %>% 
  select(date, SurfTemp_degC, Name) %>% 
  arrange(date)
#convert to a tibble
MS <- as_tibble(MS)

#both trend and alpha probably need to be played around with per lake.
MS_anom <- MS %>% 
  time_decompose(SurfTemp_degC, merge = T, trend = '3 months') %>% 
  anomalize(remainder, alpha = 0.03) %>% 
  time_recompose()

MS_anom %>% 
  filter(date > as.Date('2010-01-01'))%>% 
  plot_anomalies(time_recomposed = T)
MS_anom %>% 
  plot_anomaly_decomposition()

MS <- full_join(MS, MS_anom) %>% 
  select(Name, date, SurfTemp_degC, anomaly)
```

### Halligan Reservoir

```{r}
#grab one res and date and temp
HaR <- tempData_filtered %>% 
  filter(Name == 'Halligan Reservoir') %>% 
  select(date, SurfTemp_degC, Name) %>% 
  arrange(date)
#convert to a tibble
HaR <- as_tibble(HaR)

#both trend and alpha probably need to be played around with per lake.
HaR_anom <- HaR %>% 
  time_decompose(SurfTemp_degC, merge = T, trend = '3 months') %>% 
  anomalize(remainder, alpha = 0.02) %>% #decreasing the alpha because data are more sparse
  time_recompose()

HaR_anom %>% 
  filter(date > as.Date('2010-01-01'))%>% 
  plot_anomalies(time_recomposed = T)
HaR_anom %>% 
  plot_anomaly_decomposition()

HaR <- full_join(HaR, HaR_anom) %>% 
  select(Name, date, SurfTemp_degC, anomaly)
```

### Peterson Lake

```{r}
#grab one res and date and temp
PL <- tempData_filtered %>% 
  filter(Name == 'Peterson Lake') %>% 
  select(date, SurfTemp_degC, Name) %>% 
  arrange(date)
#convert to a tibble
PL <- as_tibble(PL)

#both trend and alpha probably need to be played around with per lake.
PL_anom <- PL %>% 
  time_decompose(SurfTemp_degC, merge = T, trend = '3 months') %>% 
  anomalize(remainder, alpha = 0.03) %>% 
  time_recompose()

PL_anom %>% 
  filter(date > as.Date('2015-01-01'))%>% 
  plot_anomalies(time_recomposed = T)
PL_anom %>% 
  plot_anomaly_decomposition()

```

The anomaly detection is not working well here - probably due to sparse data, especially at the beginning of the record. Let's do the same analysis 1999 forward, when Landsat 7 came into service.

```{r}
#grab only data after L7 launch
PL_1999 <- tempData_filtered %>% 
  filter(Name == 'Peterson Lake') %>% 
  select(date, SurfTemp_degC, Name) %>% 
  arrange(date) %>% 
  filter(date > as.Date('1999-04-01'))
#convert to a tibble
PL_1999 <- as_tibble(PL_1999)

#both trend and alpha probably need to be played around with per lake.
PL_1999_anom <- PL_1999 %>% 
  time_decompose(SurfTemp_degC, merge = T, trend = '4 months') %>% 
  anomalize(remainder, alpha = 0.05) %>% 
  time_recompose()

PL_1999_anom %>% 
  plot_anomaly_decomposition()

PL <- full_join(PL, PL_1999_anom) %>% 
  select(Name, date, SurfTemp_degC, anomaly) %>% 
  mutate(anomaly = if_else(is.na(anomaly), 'notEnoughData', anomaly))
```

### Barnes Meadow Reservoir

```{r}
#grab one res and date and temp
BMR <- tempData_filtered %>% 
  filter(Name == 'Barnes Meadow Reservoir') %>% 
  select(date, SurfTemp_degC, Name) %>% 
  arrange(date)
#convert to a tibble
BMR <- as_tibble(BMR)

#both trend and alpha probably need to be played around with per lake.
BMR_anom <- BMR %>% 
  time_decompose(SurfTemp_degC, merge = T, trend = '3 months') %>% 
  anomalize(remainder, alpha = 0.03) %>% 
  time_recompose()

BMR_anom %>% 
  filter(date > as.Date('2015-01-01'))%>% 
  plot_anomalies(time_recomposed = T)
BMR_anom %>% 
  plot_anomaly_decomposition()

```

Same issue here.

```{r}
#grab only data after L7 launch
BMR_1999 <- tempData_filtered %>% 
  filter(Name == 'Barnes Meadow Reservoir') %>% 
  select(date, SurfTemp_degC, Name) %>% 
  arrange(date) %>% 
  filter(date > as.Date('1999-04-01'))
#convert to a tibble
BMR_1999 <- as_tibble(BMR_1999)

#both trend and alpha probably need to be played around with per lake.
BMR_1999_anom <- BMR_1999 %>% 
  time_decompose(SurfTemp_degC, merge = T, trend = '4 months') %>% 
  anomalize(remainder, alpha = 0.5, max_anoms = 0.02) %>% 
  time_recompose()

BMR_1999_anom %>% 
  plot_anomaly_decomposition()

BMR <- full_join(BMR, BMR_1999_anom) %>% 
  select(Name, date, SurfTemp_degC, anomaly) %>% 
  mutate(anomaly = if_else(is.na(anomaly), 'notEnoughData', anomaly))

```

### Joe Wright Reservoir

```{r}
#grab one res and date and temp
JW<- tempData_filtered %>% 
  filter(Name == 'Joe Wright Reservoir') %>% 
  select(date, SurfTemp_degC, Name) %>% 
  arrange(date)
#convert to a tibble
JW <- as_tibble(JW)

#both trend and alpha probably need to be played around with per lake.
JW_anom <- JW %>% 
  time_decompose(SurfTemp_degC, merge = T, trend = '3 months') %>% 
  anomalize(remainder, alpha = 0.03) %>% 
  time_recompose()

JW_anom %>% 
  filter(date > as.Date('2015-01-01'))%>% 
  plot_anomalies(time_recomposed = T)
JW_anom %>% 
  plot_anomaly_decomposition()


```

And again.

```{r}
#grab only data after L7 launch
JW_1999 <- tempData_filtered %>% 
  filter(Name == 'Joe Wright Reservoir') %>% 
  select(date, SurfTemp_degC, Name) %>% 
  arrange(date) %>% 
  filter(date > as.Date('1999-04-01'))
#convert to a tibble
JW_1999 <- as_tibble(JW_1999)

#both trend and alpha probably need to be played around with per lake.
JW_1999_anom <- JW_1999 %>% 
  time_decompose(SurfTemp_degC, merge = T, trend = '4 months') %>% 
  anomalize(remainder, alpha = 0.05, max_anoms = 0.01) %>% 
  time_recompose()

JW_1999_anom %>% 
  plot_anomaly_decomposition()

JW <- full_join(JW, JW_1999_anom) %>% 
  select(Name, date, SurfTemp_degC, anomaly) %>% 
  mutate(anomaly = if_else(is.na(anomaly), 'notEnoughData', anomaly))
```

### Long Draw Reservoir

```{r}
#grab one res and date and temp
LD <- tempData_filtered %>% 
  filter(Name == 'Long Draw Reservoir') %>% 
  select(date, SurfTemp_degC, Name) %>% 
  arrange(date)
#convert to a tibble
LD <- as_tibble(LD)

#both trend and alpha probably need to be played around with per lake.
LD_anom <- LD %>% 
  time_decompose(SurfTemp_degC, merge = T, trend = '3 months') %>% 
  anomalize(remainder, alpha = 0.03) %>% 
  time_recompose()

LD_anom %>% 
  filter(date > as.Date('2015-01-01'))%>% 
  plot_anomalies(time_recomposed = T)

LD_anom %>% 
  filter(date>as.Date('2015-01-01')) %>% 
  plot_anomaly_decomposition()
```

Same.

```{r}
#grab only data after L7 launch
LD_1999 <- tempData_filtered %>% 
  filter(Name == 'Long Draw Reservoir') %>% 
  select(date, SurfTemp_degC, Name) %>% 
  arrange(date) %>% 
  filter(date > as.Date('1999-04-01'))
#convert to a tibble
LD_1999 <- as_tibble(LD_1999)

#both trend and alpha probably need to be played around with per lake.
LD_1999_anom <- LD_1999 %>% 
  time_decompose(SurfTemp_degC, merge = T, trend = '4 months') %>% 
  anomalize(remainder, alpha = 0.05) %>% 
  time_recompose()

LD_1999_anom %>% 
  plot_anomaly_decomposition()

LD <- full_join(LD, LD_1999_anom) %>% 
  select(Name, date, SurfTemp_degC, anomaly) %>% 
  mutate(anomaly = if_else(is.na(anomaly), 'notEnoughData', anomaly))
```

### Chambers Lake

```{r}
#grab one res and date and temp
CL <- tempData_filtered %>% 
  filter(Name == 'Chambers Lake') %>% 
  select(date, SurfTemp_degC, Name) %>% 
  arrange(date)
#convert to a tibble
CL <- as_tibble(CL)

#both trend and alpha probably need to be played around with per lake.
CL_anom <- CL %>% 
  time_decompose(SurfTemp_degC, merge = T, trend = '3 months') %>% 
  anomalize(remainder, alpha = 0.03) %>% 
  time_recompose()

CL_anom %>% 
  filter(date > as.Date('2015-01-01'))%>% 
  plot_anomalies(time_recomposed = T)

CL_anom %>% 
  plot_anomaly_decomposition()
```

And again.

```{r}
#grab only data after L7 launch
CL_1999 <- tempData_filtered %>% 
  filter(Name == 'Chambers Lake') %>% 
  select(date, SurfTemp_degC, Name) %>% 
  arrange(date) %>% 
  filter(date > as.Date('1999-04-01'))
#convert to a tibble
CL_1999 <- as_tibble(CL_1999)

#both trend and alpha probably need to be played around with per lake.
CL_1999_anom <- CL_1999 %>% 
  time_decompose(SurfTemp_degC, merge = T, trend = '4 months') %>% 
  anomalize(remainder, alpha = 0.02, max_anoms = 0.02) %>% 
  time_recompose()

CL_1999_anom %>% 
  plot_anomaly_decomposition()

CL <- full_join(CL, CL_1999_anom) %>% 
  select(Name, date, SurfTemp_degC, anomaly) %>% 
  mutate(anomaly = if_else(is.na(anomaly), 'notEnoughData', anomaly))
```

### Join all the data back together and export

Join the anomaly datasets

```{r}
dataForExport = full_join(HR, MS) %>% 
  full_join(., HaR) %>% 
  full_join(., PL) %>% 
  full_join(., BMR) %>% 
  full_join(., JW) %>% 
  full_join(., LD) %>% 
  full_join(., CL)
```

Join the anomaly dataset with the filtered dataset

```{r}
dataForExport <- full_join(dataForExport, tempData_filtered)
```

Look at the data again (with the anomaly flag):

```{r}
ggplot(dataForExport, aes(x = date, y = SurfTemp_degC, color = anomaly)) +
  geom_point() +
  facet_grid(Name ~ .) +
  theme_bw() +
  scale_color_colorblind()
```

Export the data

```{r}
write.csv(dataForExport, file.path(dir, 'CLP_flagged_SurfTemp_Landsat5-9_v02Dec2022.csv'), row.names = F)
```
