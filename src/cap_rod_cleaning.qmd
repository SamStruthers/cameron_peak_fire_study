---
title: "cap rod cleaning"
format: html
editor: visual
---

## Packages

```{r setup}
require("knitr")
opts_knit$set(root.dir = "~/Repositories/cameron_peak_fire_study/")
library(tidyverse)
library(lubridate)
library(ggplot2)
library(plotly)


```

## Correcting Functions

DriftR package is not up to date with Rstudio so I am sourcing the functions from a folder called driftR_functions in the src folder

```{r}
source("src/driftR_functions/dr_correctOne.R")
source("src/driftR_functions/dr_correctTwo.R")
source("src/driftR_functions/dr_drop.R")
source("src/driftR_functions/dr_factor.R")
source("src/driftR_functions/dr_read.R")
source("src/driftR_functions/dr_readSonde.R")
source("src/driftR_functions/dr_replace.R")
source("src/driftR_functions/notInOperator.R")
source("src/driftR_functions/parseTime.R")
source("src/driftR_functions/sondeCal.R")
source("src/driftR_functions/sondeClean.R")
source("src/driftR_functions/sondeRaw.R")

```

## Importing and Combining

RULES:

1.  All data must be in CSVs nested in the folders data/all_sites/

2.  All files must have naming convention: SITE\_#DATE#

3.  All files must have same number of columns and column names

4.  All files must have the same DateTime format (can be adjusted below)

5.  Run all_data first to get what the pattern for gsub will be

    1.  This is also a good way to test that none of your files are broken

6.  Use plots to double check everything

    1.  Use the plots to double check there are no zeros near site visits. If there are, remove them from the dataset manually! This will help the adjustments run correctly.

```{r}
#function testing to create one df with all caprod data
read_and_combine <- function(folder_path, full_folder_path){
  
  # This ensures that version 2.0+ of the readr package (tidyverse) is being
  # used, because it allows for reading multiple files
  readr::local_edition(2) 
  
  # Read in all data files from all
  all_data <- read_csv(
    # Provide a vector/list of filenames to read
    file = list.files(
      path = folder_path,
      # Search in all subfolders of `path` for csv files
      recursive = TRUE,
      # Use full filenames so R can find them
      full.names = TRUE),
    # Store the path to each csv in a "path" column
    id = "path")
  
  # Now tidy up the dataset before returning it to the main workflow
  cleaned_data <- all_data %>%
    mutate(
      # Create a new column with a shortened version of the path
      site_code = gsub(pattern = full_folder_path,
                     replacement = "",
                     x = path),
      # Extract just the folder name from this shortened version of the path
      site_code = str_extract(string = site_code,
      # Remove everything after the first bit of upper case text
        pattern = "[A-Z]+"),
      
      DT =  as.POSIXct(datetime, format = "%m/%d/%Y %H:%M", tz = "MST")
    )
}

## All sites combined into 1 dataframe
# change "" to the file path were all_sites is located

stage_df <- read_and_combine(folder_path = "~/Repositories/cameron_peak_fire_study/data/all_sites_caprod_data",
                                    full_folder_path = "C:/Users/Sam Struthers/OneDrive - Colostate/Documents/Repositories/cameron_peak_fire_study/data/all_sites_caprod_data/")%>%
  select(DT, site_code, wtrhgt__5)%>%
  mutate(water_height_cm = wtrhgt__5 /10, 
        time =  format(DT, format = "%H:%M"), 
        site_code = str_extract(string = site_code, 
                                      pattern = "[A-Z]+"))%>%
  dplyr ::rename(water_height_mm = wtrhgt__5)



#Pull in field measured values

manual_measurements <- read.csv("data/manual_values_dec_2022.csv")%>%
      mutate(first_measurement_DT = as.POSIXct(first_measurement_DT, format = "%m/%d/%Y %H:%M", tz = "MST"), 
             last_measurement_DT = as.POSIXct(last_measurement_DT, format = "%m/%d/%Y %H:%M", tz = "MST"))

```

## Visual

-   check for missing data, weird end dates, remove any 0s near

```{r}
master_visual <- stage_df%>%
  ggplot(aes(x= DT, y = water_height_cm, color = site_code))+
  geom_line()
plot(master_visual)

indv_visual <- filter(stage_df, site_code == "SAWM")%>%
  ggplot(aes(x= DT, y = water_height_cm, color = site_code))+
  geom_line()
plot(indv_visual)
```

## Export Masterfile

```{r}
write_csv(stage_df, "data/all_sites_combined_UNCLEAN.csv")
```

# Field data adjustments:

In broad strokes, these functions/loops will take the sensor data pulled in above and manual values measured in the field to attempt to correct sensor data to match up field data. Workflow overview:

1.  Subset sensor df by a first and second field measurement.
2.  Step sensor up or down based on the difference between first field measurement and recorded values.
3.  Create a time factor for each subset of data which will allow for the driftR package to correct for sensor drift
4.  Use driftR functions to match up end of dataset with second field measurement.

##Functions to subset and adjust stage values

```{r}
##Function to adjust sensor stage values to match start measurement  
  
  
  adjust_stage <- function(site_name,start_date, end_date, manual_start, manual_end){
    stage_df_new<- stage_df%>%
     dplyr::filter(stage_df$site_code %in% site_name)%>%
     dplyr::filter(between(DT, start_date, end_date))%>%
      mutate(adjustment = manual_start - mean(head(water_height_cm, n = 4), na.rm = TRUE) , 
             adjusted_stage = water_height_cm + adjustment)
    
    return(stage_df_new)
    
   
  }
  
##Function to give dataset a time correction factor for each subset
    dr_factor_subset <- function(site_name, start_date, end_date, manual_start, manual_end){
      
    factor_df <-stepped_df%>%
     dplyr::filter(stepped_df$site_code %in% site_name)%>%
     dplyr::filter(between(DT, start_date, end_date))%>%
      mutate(time =  format(DT, format = "%H:%M"), 
             date = as.Date(DT, format = "%Y-%m-%d"))%>%
      arrange(DT)%>%
      dr_factor(., corrFactor = corrFac, dateVar = date, timeVar = time, keepDateTime = TRUE )
    
    
      return(factor_df)
    }
    
##Function to correct drift over a subset by looking at measured values and the previous 4
## dataframe must already have corrFac added or it will not run
    
    dr_correctOne_subset <- function(site_name, start_date, end_date, manual_end, manual_start){
      
    corrected_df <-factor_df%>%
     dplyr::filter(factor_df$site_code %in% site_name)%>%
     dplyr::filter(between(DT, start_date, end_date))
    
    last_sensor_cm = mean(tail(corrected_df$adjusted_stage, n = 4),na.rm = TRUE)
    
     drift_corrected_df<- dr_correctOne(corrected_df, sourceVar = adjusted_stage, cleanVar = corrected_stage_cm ,calVal = last_sensor_cm , calStd = manual_end , factorVar = corrFac)
    
      return(drift_corrected_df)
  }
  


 



```

## Preforming the adjustments

Using the functions written above and pmap, we can map over the manual measurements df and preform the adjustments.

```{r}






#Fit column names to match argument names in functions

manual_measurements_for_func <- manual_measurements%>%
  select(site_name = site_code ,start_date = first_measurement_DT, end_date = last_measurement_DT, manual_start = first_measurement_cm, manual_end = last_measurement_cm)%>%
  na.omit()%>%
  as_tibble()





##PMAP FOR ADJUSTMENT FUNCTIONS!

#Steps df for each subset between site visits to match the first measurement (start measurement)
stepped_df <- pmap_dfr(manual_measurements_for_func, adjust_stage)

# creates a time factor for each subset using the driftR factor function
factor_df <- pmap_dfr(manual_measurements_for_func, dr_factor_subset)


#using factor and last 4 measurements from manual end ,corrects for drift so that manual end and the last measurement match up

corrected_df <- pmap_dfr(manual_measurements_for_func, dr_correctOne_subset)




  
```

## Graphing to check work

```{r}
  
indv_corrected_df <- filter(corrected_df, site_code == "LBEA")
 indv_manual_measurements <- filter(manual_measurements_for_func, site_name == "LBEA")
 
  
 indv_corrected_graph <- ggplot()+
  geom_line(data = indv_corrected_df, aes( x= DT , y= water_height_cm), color= "red")+
  geom_line(data = indv_corrected_df, aes( x= DT , y= corrected_stage_cm), color= "black")+
  geom_line(data = indv_corrected_df, aes( x= DT , y= adjusted_stage), color= "green", alpha = .5)+
  geom_point(data =  indv_manual_measurements, aes( x= start_date , y= manual_start), color = "blue")+
   theme_bw()
ggplotly(indv_corrected_graph)
plot(indv_corrected_graph)






```

## Splitting sites where caprod was moved

LBEA and SHEP had to be moved midseason so they are now being split into LBEA1/2 and SHEP1/2

This section can be skipped if sensors did not move location thru the season

```{R}



# This dataframe contains the old names, new names and star/end dates for the name changes
# If there are more moves, this can be written in as a csv with the same  column names and read in using the hashed out code below

rename_df <- data.frame(old_name = c("LBEA", "LBEA", "SHEP","SHEP"), 
                       new_name = c("LBEA1", "LBEA2", "SHEP1", "SHEP2"), 
                       start_date = c("2022-05-01 12:00","2022-08-19 10:15","2022-05-01 12:00","2022-08-19 12:30"), 
                       end_date= c("2022-08-19 10:00","2022-11-01 12:00","2022-08-17 15:15","2022-11-01 12:00"))%>% mutate(start_date = as.POSIXct(start_date, format = "%Y-%m-%d %H:%M", tz = "MST"),
         end_date = as.POSIXct(end_date, format = "%Y-%m-%d %H:%M", tz = "MST"))

# rename_df <- read.csv("FILE_PATH_TO.CSV")%>% 
#mutate(start_date = as.POSIXct(start_date, format = "%Y-%m-%d %H:%M", tz = "MST"),
#          end_date = as.POSIXct(end_date, format = "%Y-%m-%d %H:%M", tz = "MST"))


change_site_names <- function(old_name, new_name, start_date, end_date){
  test_function <- corrected_df%>%
    dplyr:: filter(site_code == old_name & between(DT, start_date, end_date) )%>%
    mutate(site_code = new_name)
  
  return(test_function)
}

non_name_changed_df = corrected_df[!(corrected_df$site_code %in% unique(rename_df$old_name)),]

corrected_df<- pmap_dfr(rename_df, change_site_names)%>%
  rbind(non_name_changed_df)

#Check that all the new names are in df and old names removed
unique(corrected_df$site_code)
#If old names persist you may need to change the start/end dates so that all rows are included


```

##Add flag type column and associated title

Flags were determined visually and compared to field notes about the streams.

```{r}


flag_values<- read.csv("data/flag_values_caprods_2022.csv")%>%
  mutate(start_dt = as.POSIXct(start_dt, format = "%m/%d/%Y %H:%M", tz = "MST"),
         end_dt = as.POSIXct(end_dt, format = "%m/%d/%Y %H:%M", tz = "MST"))

apply_flag <- function(site, FLAG, start_dt, end_dt, notes){
correct_and_flagged_df <- corrected_df%>%
  dplyr::filter(corrected_df$site_code %in% site)%>%
  dplyr::filter(between(DT,start_dt, end_dt))%>%
  mutate(flag_type = FLAG, 
         note = notes)

return(correct_and_flagged_df)
  
}

correct_and_flagged_df <- pmap_dfr(flag_values, apply_flag)

flag_colors <- c("PASS" = "green", "FAIL" = "red", "TREND" = "purple", "VARIATION"= "blue", "CHANNEL"= "orange") 

indv_site <- correct_and_flagged_df%>%
  filter(site_code == "BLAK")



indv_site_flags <- ggplot()+
  geom_point(data = indv_site, aes(x = DT, y= corrected_stage_cm, color = flag_type ))+
  scale_color_manual(values = flag_colors)+
  theme_bw()
                     
plot(indv_site_flags)                     
                    
                     
```

## Exporting 15min finalized data

```{r}
final_stage_df<- correct_and_flagged_df%>%
  select(DT, site_code, 
         sensor_stage_mm = water_height_mm, 
         sensor_stage_cm = water_height_cm, 
         adjustment_cm = adjustment,
         adjusted_stage_cm = adjusted_stage,
         time_correction_value = corrFac, 
         corrected_stage_cm,
         flag_type, 
         notes = note,
         date, time)

write.csv(final_stage_df, "data/corrected_15min_stage_CPF_2022.csv")

```

## Computing daily averages

```{r}

daily_means_stage <- corrected_df%>%
  select(water_height_cm, corrected_stage_cm, adjusted_stage, date, site_code)%>%
  group_by(date, site_code)%>%
  dplyr:: summarise(mean_sensor_stage_cm = mean(water_height_cm),
         mean_adjusted_stage_cm = mean(adjusted_stage),
         mean_corrected_stage_cm = mean(corrected_stage_cm))%>%
  ungroup()%>%
   ##IGNORE UNLESS YOU NEED TO MATCH GRAB SAMPLES UP TO SITES
  mutate(site_label = ifelse(site_code %in% c("LBEA1", "LBEA2"), "LBEA",
                      ifelse(site_code %in% c("SHEP1", "SHEP2"), "SHEP",site_code)))
  ##HASH OUT EVERYTHING ABOVE IF NOT MATCHING SAMPLES AND 

#Match up with FCW and ISCO Samples 

site_labels <- c("FISH", "BEAV", "BENN", "SAWM", "PENN", "BLAK", "SHEP", "LBEA", "ROAR", "SAWM_ISCO", "FISH_ISCO", "LBEA_ISCO", 'BLAK_ISCO', "LBEAISCO", "BLAKISCO", "SAWMISCO", "FISHISCO" )

cam_pk_chem <- read.csv("data/CamPkChem.csv")%>%
  filter(Era == "FCW" | Era == "ISCO")%>%
  dplyr::rename(site_label = SiteLabel)%>%
  filter(site_label %in% site_labels)%>%
  mutate( date  = as.Date(Date, format = "%d-%b-%y"), 
          Year = year(date), 
          dayofyear = yday(date))%>%
  filter(Year >=2022)%>%
  filter(SampleType %in% c("NORM", "ISCO"))%>%
  select(date, site_label, Era, EraSamp, dayofyear, SampleType)%>%
mutate(site_label=ifelse(site_label %in% c('FISH','FISH_ISCO'),"FISH",
                   ifelse(site_label %in% c('LBEA','LBEA_ISCO', "LBEAISCO"),"LBEA",
                   ifelse(site_label %in% c('SAWM','SAWM_ISCO'),"SAWM",
                   ifelse(site_label %in% c('BLAK','BLAK_ISCO'),"BLAK",site_label )))))



sample_by_day <- cam_pk_chem%>%
  group_by(site_label, date, SampleType)%>%
  dplyr::summarise( count = n())%>%
  ungroup()

#create df of sites and codes for data flag but on a daily timescale

flag_daily_values<- flag_values%>%
  mutate(start_date = as.Date(start_dt), 
         end_date = as.Date(end_dt))%>%
  select(site, FLAG, start_date, end_date, notes)

#changed apply_flag to work on daily timestep rather than by DT

apply_flag_daily <- function(site, FLAG, start_date, end_date, notes){
correct_and_flagged_daily_df <- daily_means_stage%>%
  dplyr::filter(between(date,start_date, end_date))%>%
  dplyr::filter(site_code == site)%>%
  mutate(flag_type = FLAG, 
         note = notes)

return(correct_and_flagged_daily_df)
  
}




correct_and_flagged_daily_df <- pmap_dfr(flag_daily_values, apply_flag_daily)



daily_means_stage_w_samples <- correct_and_flagged_daily_df%>%
  left_join( select(sample_by_day, c(site_label, date, count,SampleType )), by = c("site_label", "date") )



sample_plot <- filter(daily_means_stage_w_samples, site_code == "FISH")%>%
  ggplot()+
  geom_line(aes(x= date, y = mean_corrected_stage_cm, group = site_code, color = flag_type))+
  geom_line(aes(x= date, y = mean_sensor_stage_cm, group = site_code, color = flag_type))+
  scale_color_manual(values = flag_colors)+
  #geom_line(aes(x=date, y = mean_corrected_stage_cm, color = "red" ))+
  geom_point(aes(x= date, y = count, color = "grey" ))

plot(sample_plot)

```

##Exporting corrected daily means data

```{r}

#Write daily averages file matched up with sample #

final_daily_means_stage <- daily_means_stage_w_samples%>%
  select(date,site_code, mean_sensor_stage_cm, mean_adjusted_stage_cm, mean_corrected_stage_cm,SampleType ,sampleCount = count, flag_type, note, site_label)

write_csv(final_daily_means_stage, "data/stage_daily_mean_and_samples_cleaned.csv")



```

#Calculating Q

This section contains 2 ways to calculate rating curves:
1. For datasets where there are few measurements, the function Q ~ (a*H)^b

2. For datasets with more Q/stage measurements, the package "bdrc" may be a more accurate solution
  This package calculates more error guidelines but takes a while to run. 

## Playing with rating curve package 'bdrc'

FOR LARGER DATASETS USE THIS PACKAGE. 
NEEDS FUNCTION DEVELOPMENT TO MAP THROUGH MULTIPLE SITES
CODE IN CHUNK IS EXAMPLE DATASET PROVIDED WITH PACKAGE

```{r}

library("bdrc")

data(krokfors)

gplm.fit <- gplm(Q~W,krokfors)

summary(gplm.fit)

 plot(gplm.fit)

```
 



##Functions for Q calculations
```{r}

create_rating_curve <- function(site){
  
  site_manual_h_q <- h_q_df%>%
    dplyr::filter(site_code == site)%>%
    mutate(logH = log10(H),
           logQ = log10(Q))
  
  
  # create linear model based on log of both functions
  rc_lm <- lm(logQ ~ logH, data = site_manual_h_q )
  
  #summary of lm created
  rc_summary <- summary(rc_lm)
  
    site_manual_h_q<- site_manual_h_q%>%
  mutate(
         a = (10^(rc_summary$coefficients[1,1])),
           b = rc_summary$coefficients[2,1],
           r2 = rc_summary$r.squared,
         Q_cfs = (a*(H)^b) )

  return(site_manual_h_q)
}



calc_Q_for_sensor_stage_15min <- function(site){
  
  site_rc <- manual_H_Q_rc%>%
    dplyr::filter(site_code == site)
  
  #filter corrected stage df for site
   stage_Q_df <- final_stage_df%>%
     dplyr::filter(site_code == site)%>%
     #grab a, b, c and sigma for the given site from rc summary
     mutate( a = site_rc$a[1],
           b = site_rc$b[1],
           r2 = site_rc$r2[1],
            #calculate Q for the site based on a,b,c and stage given
            Q_cfs = 
              #If the stage is below the lowest stage where Q was measured, make it the lowest Q value measured
              #extrapolation with this few of values is unwise
              ifelse(corrected_stage_cm < min(site_rc$H),  min(site_rc$Q),
             #Similar Idea to above but with max stage measured        
              ifelse(corrected_stage_cm >max(site_rc$H),  max(site_rc$Q),
             #if it within the bounds of measured stage values, use a, b to calc Q        
                     
     (a*(corrected_stage_cm)^b))))
  
  
return(stage_Q_df)
}

calc_Q_for_sensor_stage_daily <- function(site){
  
  site_rc <- manual_H_Q_rc%>%
    dplyr::filter(site_code == site)
  
  #filter corrected stage df for site
   daily_stage_Q_df <- final_daily_means_stage%>%
     dplyr::filter(site_code == site)%>%
     #grab a, b, c and sigma for the given site from rc summary
     mutate( a = site_rc$a[1],
           b = site_rc$b[1],
           r2 = site_rc$r2[1],
            #calculate Q for the site based on a,b,c and stage given
            Q_cfs = 
              #If the stage is below the lowest stage where Q was measured, make it the lowest Q value measured
              #extrapolation with this few of values is unwise
              ifelse(mean_corrected_stage_cm < min(site_rc$H),  min(site_rc$Q),
             #Similar Idea to above but with max stage measured        
              ifelse(mean_corrected_stage_cm >max(site_rc$H),  max(site_rc$Q),
             #if it within the bounds of measured stage values, use a, b to calc Q        
                     
     (a*(mean_corrected_stage_cm)^b))))
  
  
return(daily_stage_Q_df)
}


EXTRAPOLATION_Q_sensor_15min <- function(site){
  
  site_rc <- manual_H_Q_rc%>%
    dplyr::filter(site_code == site)
  
  #filter corrected stage df for site
   extrapolated_stage_Q_df <- df_for_q_test%>%
     dplyr::filter(site_code == site)%>%
     #grab a, b, c and sigma for the given site from rc summary
     mutate( a = site_rc$a[1],
           b = site_rc$b[1],
           r2 = site_rc$r2[1],
            #calculate Q for the site based on a,b,c and stage given
            Q_cfs = (a*(corrected_stage_cm)^b))
  
  
return(extrapolated_stage_Q_df)
}

EXTRAPOLATION_Q_sensor_daily <- function(site){
  
  site_rc <- manual_H_Q_rc%>%
    dplyr::filter(site_code == site)
  
  #filter corrected stage df for site
   daily_extrapolated_stage_Q_df <- final_daily_means_stage%>%
     dplyr::filter(site_code == site)%>%
     #grab a, b, c and sigma for the given site from rc summary
     mutate( a = site_rc$a[1],
           b = site_rc$b[1],
           r2 = site_rc$r2[1],
            #calculate Q for the site based on a,b,c and stage given
            Q_cfs = (a*(mean_corrected_stage_cm)^b))
  
  
return(daily_extrapolated_stage_Q_df)
}



```


## Doing Q calculations

Reading in manual measurements and bringing in final_stage_df

Steps:
1) Run through all manual measurements to create rating curve model and save coefficients
2) Double check that measured and modeled Q are in acceptable ranges
- RERUN WITHOUT CERTAIN MEASUREMENTS IF NECESSARY
- USE BEST JUDGEMENT HERE
3) Run modeled rating curves over sensor stage at 15min interval and daily interval
  Q IS BOUNDED AT UPPER AND LOWER MANUAL MEASUREMENTS.
  IF Q IS DESIRED TO BE EXTRAPOLATED, USE EXTRAPOLATION_Q_sensor_15min and EXTRAPOLATION_Q_sensor_daily (not recommended)
  
  
###Manual stage/Q import, cleaning and model creation  

```{r}

#Import csv with measurement DT, stage H and measured Q

manual_h_q <- read.csv("data/manual_stage_Q_jan2023.csv")%>%
  mutate(DT = as.POSIXct(DT, format = "%m/%d/%Y %H:%M", tz = "MST"))%>%
  na.omit(Q_cfs)


### If site was split can use this section
change_name_manual <-function(old_name, new_name, start_date, end_date){
  renamed_manual_df <- manual_h_q%>%
    dplyr:: filter(site_code == old_name & between(DT, start_date, end_date) )%>%
    mutate(site_code = new_name)
  
  return(renamed_manual_df)
}

 
#unchanged sites
manual_h_q_nochange = manual_h_q[!(manual_h_q$site_code %in% unique(rename_df$old_name)),]

#Fixed and combine manual H/Q df
h_q_df <- pmap_dfr(rename_df, change_name_manual)%>%
  rbind(manual_h_q_nochange)%>%
  select(H = H_cm, Q = Q_cfs,site_code )

###

#If sites were not split, run code below
#h_q_df <- manual_h_q%>%
#select(H = H_cm, Q = Q_cfs,site_code )

#List sites that we want to calculate rating curve for
site_list <- data.frame(site = c("FISH", "SAWM", "BLAK", "PENN", "BEAV", "BENN", "LBEA1", "SHEP1"))%>%
  as_tibble()

#using manual dataframe to create coefficients for each site
manual_H_Q_rc <- pmap_dfr(site_list, create_rating_curve)

#double check that modelled Q matches measured Q
double_check_model <- manual_H_Q_rc%>%
  ggplot()+
  geom_point(aes(x= H, y = Q), color= "blue")+
  geom_point(aes(x= H, y = Q_cfs), color= "red")+
  geom_line(aes(x= H, y = Q_cfs), color= "green")+
  theme_bw()+
  facet_wrap(~site_code)

plot(double_check_model)  
  
  
```
  



### 15 min Q calculations
```{r}

stage_Q_15min <- pmap_dfr(site_list, calc_Q_for_sensor_stage_15min)


double_check_stage_Q <- filter(stage_Q_15min, site_code == "LBEA1")%>%
  ggplot()+
  geom_line(aes(x= DT, y = Q_cfs, group = site_code, color = flag_type))+
  scale_color_manual(values = flag_colors)+
  theme_bw()

plot(double_check_stage_Q)



###DANGER ZONE###

#EXTRAPOLATED_stage_Q_15min <- pmap(site_list, EXTRAPOLATION_Q_sensor_15min)


```


### Daily Q calculations
```{r}

stage_Q_daily <- pmap_dfr(site_list, calc_Q_for_sensor_stage_daily)




double_check_stage_Q <- filter(stage_Q_daily, site_code == "LBEA1")%>%
  ggplot()+
  geom_line(aes(x= date, y = Q_cfs, group = site_code, color = flag_type))+
  scale_color_manual(values = flag_colors)+
  theme_bw()

plot(double_check_stage_Q)



###DANGER ZONE###

EXTRAPOLATED_stage_Q_daily <- pmap_dfr(site_list, EXTRAPOLATION_Q_sensor_daily)


extrapolated_stage_Q <-
  ggplot()+
  geom_line(data= filter(stage_Q_daily, site_code == "SAWM"),aes(x= date, y = Q_cfs, group = site_code, color = "blue"))+
  geom_line(data= filter(EXTRAPOLATED_stage_Q_daily, site_code == "SAWM"),aes(x= date, y = Q_cfs, group = site_code, color = "red"))+
  scale_color_manual(values = flag_colors)+
  theme_bw()

plot(extrapolated_stage_Q)


```