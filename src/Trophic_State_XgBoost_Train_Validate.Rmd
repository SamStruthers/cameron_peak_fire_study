---
title: "Chl-a Model : Optical Only (XgBoost)"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Workflow
```{r}

## 1) Read in training file ; filter, calculate band ratios, and remove correlated variables 


## 2) Set up train / test splits for random CV 


## 3) Hypertune xgboost parameters and save as 'best_params' 


## 4) Train final model with best params , look at evaluation metrics from test data

```

Packages
```{r}
library(tidyverse)
library(xgboost)
library(caret)
library(ggplot2)
library(htmlTable)
library(extrafont)
library(colorscience)
library(Metrics)
```

1) Read in training file ; filter, calculate band ratios, and remove correlated variables 
```{r}
#training file 
training <- read.csv("C:/Users/SamSillen/Desktop/WesternMountainsChlPreds-main/csvs/training.csv") 

#add function to calculate dominant wavelength
#you don't need this if your training file already has a dWL field
source('C:/Users/SamSillen/Desktop/WesternMountainsChlPreds-main/ML_utils.R')

#insert here whatever filtering you would like (i.e. p count >= x)

training <- training %>%
    filter(pixelCount > 9, 
         clouds < 50,    
         across(c(blue, green, red, nir, swir1, swir2), ~ .x > 0 & .x < 2000)) %>% # taken from Simon Code (reasonable reflectance values)
  mutate(dWL = fui.hue(red, green, blue), #this line isn't necessary if you already have dWL as a field
         red_to_blue = red/blue,
         red_to_nir = red/nir,
         nir_to_red = nir/red,
         blue_to_green = blue/green,
         green_to_blue = green/blue,
         blue_min_red_ovr_green = (blue-red)/(green),
         nir_sac = nir-swir1,
         nir_sac2 = nir-1.03*swir1,
         nir_min_red = nir-red,
         id = row_number())

#Now I will select all the optical variables in my training file, and determine which ones are correlated. I will only use uncorrelated vars as training features in my model 

optical_data <- training %>%
  select(blue, green, red, dWL, red_to_blue, red_to_nir, nir_to_red, blue_to_green, green_to_blue, blue_min_red_ovr_green, nir_sac, nir_sac2, nir_min_red)

#get correlation matriz
corr.matrix <- cor(optical_data)

#r of 0.9 is the cutoff, it's the default value I believe
corr <- findCorrelation(corr.matrix, cutoff = 0.9)

hc = sort(corr)

#reduced data is are the uncorrelated variables that we are left with
reduced_Data = corr.matrix[,-c(hc)]

# The uncorrelated varsare: blue,  dwl, red_to_blue,red_to_nir, nir_to_red, green_to_blue, nir_sac, nir_min_red

#only select uncorrelated optical vars and climate data to use as training data

data <- training %>%
  select(blue, dWL, nir, swir2, red_to_blue, red_to_nir, nir_to_red, green_to_blue, nir_sac, nir_min_red, chl_a, lagoslakeid,id, mean_temp_14_day, date, wind)

#Xgboost won't take data with NAs, so drop them if you have any or check for them 
data <- data %>%
  drop_na()

```

2) Set up train / test splits for random CV 
```{r}
#response variable 
target <- 'chl_a'

#predictor vars or 'training features'
#temp and wind won't be applicable for models over rivers
feats <- c("blue", "dWL", "nir", "swir2", "red_to_blue", "red_to_nir", "nir_to_red", "green_to_blue", "nir_sac", "nir_min_red", "mean_temp_14_day","wind")

#for reproducible model outputs
set.seed(100)

#this is for random validation, set aside 80 % of our training dataset for trainig
train <- data %>% sample_frac(0.8)

#the rest, our holdout (20%) of our training set for model testing
test <- data %>% 
  filter(!id %in% train$id)

#create xgb matrix for train data
dtrain <- xgb.DMatrix(data = as.matrix(train[feats]), label = train[target][[1]])

#create xgb matrix for test data
dtest <- xgb.DMatrix(data = as.matrix(test[feats]), label = test[target][[1]])

```


3) Hypertune xgboost parameters and save as 'best_params' 
```{r}

#create grid of hyperparamters that are "conservative"

grid_train <- expand.grid(
  max_depth= c(2,3,4), #max depth is Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit.
  subsample = c(.5,.8,1), #subsample describes the subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees, and this would prevent overfitting.
  colsample_bytree= c(.5,.8,1), #col sample by tree specify the fraction of columns to be subsampled.
  eta = c(.01, 0.1), #eta shrinks the feature weights of new features to make the boosting process more conservative.
  min_child_weight= c(1,3,5) #not sure about min child weight actually, check out : https://xgboost.readthedocs.io/en/stable/parameter.html for more information 
)

#below is a function that select hyperparamters that result in the lowest model error (log loss)
hypertune_xgboost = function(train,test, grid){
  
  params <- list(booster = "gbtree", objective = 'reg:squaredlogerror', eta=grid$eta ,max_depth=grid$max_depth, 
                 min_child_weight=grid$min_child_weight, subsample=grid$subsample, colsample_bytree=grid$colsample_bytree)
  
  xgb.naive <- xgb.train(params = params, data = dtrain, nrounds = 2000, 
                         watchlist = list(train = train, val = test), 
                         print_every_n =100, early_stopping_rounds = 20)
  
  summary <- grid %>% mutate(val_loss = xgb.naive$best_score, best_message = xgb.naive$best_msg)
  
  return(summary) 
}

## Hypertune xgboost
xgboost_hypertune <- grid_train %>%
  pmap_dfr(function(...) {
    current <- tibble(...)
    hypertune_xgboost(dtrain,dtest,current)
  })

#pull out best params

best_params <- xgboost_hypertune[xgboost_hypertune$val_loss==min(xgboost_hypertune$val_loss),]

best_params <- list(booster = "gbtree", objective = 'reg:squaredlogerror',
               eta=best_params$eta,
               max_depth=best_params$max_depth, 
               min_child_weight=best_params$min_child_weight, 
               subsample=best_params$subsample, 
               colsample_bytree=best_params$colsample_bytree)


```


4) Re-train final model with best params , look at evaluation metrics from test data
```{r}
#train final model with best params
final_model <- xgb.train(params = best_params, data = dtrain, nrounds = 2000, print_every_n = 20)

#evaluate model using trest data
test <- test %>%
  mutate(chl_predicted = predict(final_model, dtest))

evals <- test %>%
  mutate(Actual = (chl_a), 
         Predicted = (chl_predicted)) %>%
  summarise(rmse = rmse(Actual, Predicted),
            mae = mae(Actual, Predicted),
            mape = mape(Actual, Predicted),
            bias = bias(Actual, Predicted),
            p.bias = percent_bias(Actual, Predicted),
            smape = smape(Actual, Predicted)) 

ggplot(test, aes(x = chl_a, y = chl_predicted)) +
  geom_point()+
  xlim(0, 100) + #I'm filtering out the really high chl_a obs for visualization, but would probably do this earlier on before training the model
  ylim(0, 100) + #Now I'm making the axes the same size 
  theme_bw() +
  labs(x = 'Observed') +
  labs(y = 'Predicted') + 
  theme(axis.text.x=element_text(size=15, colour =  'black'),panel.grid.minor = element_blank(), panel.grid.major = element_blank(), axis.text.y = element_text(size=15,colour = 'black'), axis.title.x =     element_text(size=20, face = 'bold'),axis.title.y = element_text(size=20, face = 'bold'), legend.title = element_text(size = 15), legend.text = element_text(size = 15))

```

#Explore feature importance
```{r}
imp <- xgb.importance(model = final_model)

xgb.plot.importance(importance_matrix = imp)
```

#Using the model to predict
```{r}

#To use the model to predict on the full sf dataset (i.e. Limnosat) , read in sf, then make it a matrix, then use predict()

#Below is an example

sf <- read.csv("C:/Users/samsi/Desktop/sf.csv")


dtest <- xgb.DMatrix(data = as.matrix(sf.final.v3[feats]))


sf.test <- sf.final.v3 %>%
  mutate(trophic_predicted = predict(final_model, dtest))


```