---
title: "Larimer County Downloader"
author: "Katie Willi and Sam Struthers"
date: "`r Sys.Date()`"
output: html_document
---

# Larimer County Data Puller

This workflow will grab all the available precipitation and discharge from the site:

<https://larimerco-ns5.trilynx-novastar.systems/novastar/operator/#/map>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(jsonlite)
library(httr)
library(arrow)
source("package_loader.R")
```

# Metadata Grabber

```{r}

site_pull <- GET(url = "https://larimerco-ns5.trilynx-novastar.systems/novastar/data/api/v1/stationSummaries?forOperatorStationSummary=true")
site_content <- httr::content(site_pull)


final_meta <- tibble()
no_station_data <- tibble()

## Functionalize the loop below and return a df named final_meta with all the station data
## and a df named no_station_data with all the stations that don't have any data



  for(i in 1:length(site_content[["stationSummaries"]])){
  
    column_names <- c("id", "numId", "name", "elevation", "lat", "long" )
    
 basics <- site_content[["stationSummaries"]][[i]] %>% 
    rbind() %>% 
    as_tibble() %>%
    dplyr::select(id, numId, name, elevation, lat = latitude, long = longitude)%>%
    ## make all data characters rather than lists or factors
      mutate_at(vars(id, numId, name, elevation, lat, long), as.character)

## write code to skip station that doesn't have any dataType data 
  if(length(site_content[["stationSummaries"]][[i]][["dataTypes"]]) == 0){
    no_station_data <- bind_rows(no_station_data, basics)
    next
  }

  station_meta <- as.data.frame(do.call(rbind, 
                                        site_content[["stationSummaries"]][[i]][["dataTypes"]] ))%>%
    mutate(dataType = as.character(name)) %>%
    distinct(dataType)%>%
    rowid_to_column()%>%
    cbind(basics)%>%
    select(dataType, numId, id, name, lat, long, rowid)%>%
    pivot_wider(., names_from = dataType, values_from = rowid )%>%
    as_tibble()
  
  final_meta <- bind_rows(final_meta, station_meta)
  }

## Use  sf to transform at the locations of the sites in look final and no_station_data
## remove NAs from lat and long columns from look final and no_station_data 



final_meta <- final_meta%>%
  filter(!is.na(lat) & !is.na(long)& lat != "NULL")
no_station_data <- no_station_data%>%
  filter(!is.na(lat)& !is.na(long)& lat != "NULL")

  

  ## create columns that classify sites by agency using the name column going off the list "Larimer", "Fort Collins", "Greeley" and "USGS"
  # Label other sites as "Other" in the name column
  final_meta <- final_meta%>%
    mutate(agency = case_when(
      grepl("Larimer -", name) ~ "Larimer",
      grepl("Fort Collins -", name) ~ "Fort Collins",
      grepl("Greeley -", name) ~ "Greeley",
      grepl("USGS", name) ~ "USGS",
      grepl("Loveland -", name) ~ "Loveland",
      grepl("Weld - ", name) ~ "Weld",
      grepl("Windsor - ", name) ~ "Windsor",
      TRUE ~ "Other"
    ))

    #remove the agency name from the name column
    final_meta <- final_meta%>%
      mutate(name = gsub("Larimer - ", "", name))%>%
      mutate(name = gsub("Fort Collins - ", "", name))%>%
      mutate(name = gsub("Greeley - ", "", name))%>%
      mutate(name = gsub("USGS ", "", name))%>%
      mutate(name = gsub("Loveland - ", "", name))%>%
      mutate(name = gsub("Weld - ", "", name))%>%
      mutate(name = gsub("Windsor - ", "", name))


## Count the number of sites by agency
final_meta%>%
  group_by(agency)%>%
  count()%>%
  arrange(desc(n))

## Classify sites by data_available, if the site has data in the dischargeRiver and Precip columns, then it is classified as "both". If it has only dischargeRiver data, then it is classified as "discharge". If it has only Precipitation data, then it is classified as "precipitation". If it has neither, then it is classified as "none"
final_meta <- final_meta%>%
  mutate(data_available = case_when(
    !is.na(DischargeRiver) & !is.na(Precip) ~ "Precip and Q",
    !is.na(DischargeRiver) & is.na(Precip) ~ "Q",
    is.na(DischargeRiver) & !is.na(Precip) ~ "Precip",
    is.na(DischargeRiver) & is.na(Precip) ~ "none"
  ))%>%
  rename(Q = DischargeRiver, 
         stage = WaterLevelRiver)



```

## Map sites:

```{r}

## use mapview to look at the sites in both dataframes

final_meta_sf <- final_meta%>%
  mutate(lat = as.numeric(lat),
         long = as.numeric(long))%>%
  st_as_sf(coords = c("long", "lat"), crs = 4326)

no_station_data_sf <- no_station_data%>%
mutate(lat = as.numeric(lat),
         long = as.numeric(long))%>%
  st_as_sf(coords = c("long", "lat"), crs = 4326)

  station_map <- mapview::mapview(final_meta_sf, zcol = "data_available", alpha.regions = 0.5, map.types = "Esri.WorldTopoMap")


  station_map



```

### Export location data

of all sites that have Q or precipitation

```{r}
## create a dataframe and csv that has the lat, long, site name, agency and data available for each site from final_meta

final_meta%>%
  filter(data_available != "none")%>%
  select(name,numId, lat, long, agency, data_available)%>%
write_csv("data/Q_modeling/station_data.csv")
```

# Downloading data

Parameters available are:

\- Q

\- Precip

-   Stage

## Downloader function

```{r download function}

data_downloader <- function(site_nums, parameter){
  

  url <- paste0("https://larimerco-ns5.trilynx-novastar.systems/novastar/data/api/v1/",
                "stationSummaries?forOperatorStationDashboard=true&stationNumId=", site_nums,
                "&periodStart=2015-01-25T13:59:00-07:00&periodEnd=", Sys.Date(),
                "T13:59:00-07:00")
  
  request <- httr::GET(url = url)
  
  total_list <- httr::content(request)
  
  parameter_row <- final_meta%>%
    filter(numId == site_nums)%>%
    pull(parameter)%>%
    as.numeric()
  

  
  unlister <- function(parameter_row){

    
    unlisted <- total_list[["stationSummaries"]][[1]][["ts"]][[parameter_row]][["data"]] %>%
      bind_rows() %>%
      mutate(locId = site_nums) %>%
      as.data.frame()
    
    return(unlisted)
    
}
  
  data <- parameter_row %>%
    map(~ unlister(parameter_row= .)) %>%
    dplyr::bind_rows()%>%
    mutate(param = parameter)
  
  return(data)
  
}
```

## Q:

This section will download all the Q from the date set in the function up until the system date. The longer the time frame, the longer the download will take.

### Download:

```{r}

# Pick out site numbers for Q locations
site_nums <- final_meta%>%
  filter(!is.na(Q))%>%
  select(numId)%>%
  pull()
# set parameter
parameter <- "Q"

#Map over all Q sites and consolidate into single dataframe
q_for_all <- map2(site_nums, parameter, data_downloader) %>%
  list_rbind()%>%
  mutate(datetime = lubridate::ymd_hms(dt))

#check to make sure that all sites were included

unique(q_for_all$locId)

#pick out all metadata for Q sites

meta_for_join <- final_meta%>%
  filter(!is.na(Q))%>%
  mutate(locId = as.character(numId))%>%
  distinct(locId, .keep_all = TRUE)%>%
  select(locId, id, name, lat, long, agency, data_available)

#join with larger dataframe 
q_for_all_wmeta <- q_for_all%>%
  left_join(meta_for_join, by = "locId")%>%
  mutate(q_cfs = as.numeric(v)) 


## Left out USGS Fort Collins site and loveland site wrq that has no Q
```

### Check the Q

I'd recommend going to <https://larimerco-ns5.trilynx-novastar.systems/novastar/operator/#/stationDashboard/100> to double check and make sure the data looks similar to the sites you pulled

```{r}
#plot all sites
ggplot(data = q_for_all_wmeta ) +
  geom_point(aes(x = datetime, y = q_cfs)) +
  theme_bw() +
  facet_wrap(~locId)
#plot indv site
ggplot(data =filter( q_for_all_wmeta,locId == "3530") ) +
  geom_point(aes(x = datetime, y = q_cfs)) +
  theme_bw()

```

### Exporting Q to arrow

```{r}

write_feather(q_for_all_wmeta, sink = "data/Q_modeling/larimer_co_2015-01-25_2023-01-30")

```

## Precipitation:

### Download:

```{r}

# Pick out site numbers for Precip locations
site_nums <- final_meta%>%
  filter(!is.na(Precip))%>%
  select(numId)%>%
  pull()
# set parameter
parameter <- "Precip"

#Map over all Q sites and consolidate into single dataframe
p_for_all <- map2(site_nums, parameter, data_downloader) %>%
  list_rbind()%>%
  mutate(datetime = lubridate::ymd_hms(dt))

#check to make sure that all sites were included

unique(p_for_all$locId)

#pick out all metadata for Q sites

meta_for_join <- final_meta%>%
  filter(!is.na(Precip))%>%
  mutate(locId = as.character(numId))%>%
  distinct(locId, .keep_all = TRUE)%>%
  select(locId, id, name, lat, long, agency, data_available)

#join with larger dataframe 

p_for_all_wmeta <- p_for_all%>%
  left_join(meta_for_join, by = "locId")%>%
  mutate(p_mm = as.numeric(v)*25.4) 

```

### Check Precip

I'd recommend going to <https://larimerco-ns5.trilynx-novastar.systems/novastar/operator/#/stationDashboard/100> to double check and make sure the data looks similar to the sites you pulled.

```{r}
#plot all sites
ggplot(data = p_for_all_wmeta ) +
  geom_point(aes(x = datetime, y = p_mm)) +
  theme_bw() +
  facet_wrap(~locId)

#plot indv site
ggplot(data =filter( p_for_all_wmeta,locId == "3530") ) +
  geom_point(aes(x = datetime, y = p_mm)) +
  theme_bw()


```

### Export Precip to arrow

```{r}

write_feather(p_for_all_wmeta, sink = "data/Q_modeling/larimer_co_precip_2015-01-25_2023-01-30")

```
