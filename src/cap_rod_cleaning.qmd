---
title: "cap rod intake"
format: html
editor: visual
---

## Packages

```{r setup}
require("knitr")
opts_knit$set(root.dir = "~/Repositories/cameron_peak_fire_study/")
library(tidyverse)
library(lubridate)
library(ggplot2)
library(plotly)


```

## Correcting Functions

DriftR package is not up to date with Rstudio so I am sourcing the functions from a folder called driftR_functions in the src folder

```{r}
source("src/driftR_functions/dr_correctOne.R")
source("src/driftR_functions/dr_correctTwo.R")
source("src/driftR_functions/dr_drop.R")
source("src/driftR_functions/dr_factor.R")
source("src/driftR_functions/dr_read.R")
source("src/driftR_functions/dr_readSonde.R")
source("src/driftR_functions/dr_replace.R")
source("src/driftR_functions/notInOperator.R")
source("src/driftR_functions/parseTime.R")
source("src/driftR_functions/sondeCal.R")
source("src/driftR_functions/sondeClean.R")
source("src/driftR_functions/sondeRaw.R")

```

## Importing and Combining

RULES:

1.  All data must be in CSVs nested in the folders data/all_sites/

2.  All files must have naming convention: SITE\_#DATE#

3.  All files must have same number of columns and column names

4.  All files must have the same DateTime format (can be adjusted below)

5.  Run all_data first to get what the pattern for gsub will be

    1.  This is also a good way to test that none of your files are broken

6.  Use plots to double check everything

    1.  Use the plots to double check there are no zeros near site visits. If there are, remove them from the dataset manually! This will help the adjustments run correctly.

```{r}
#function testing to create one df with all caprod data
read_and_combine <- function(folder_path, full_folder_path){
  
  # This ensures that version 2.0+ of the readr package (tidyverse) is being
  # used, because it allows for reading multiple files
  readr::local_edition(2) 
  
  # Read in all data files from all
  all_data <- read_csv(
    # Provide a vector/list of filenames to read
    file = list.files(
      path = folder_path,
      # Search in all subfolders of `path` for csv files
      recursive = TRUE,
      # Use full filenames so R can find them
      full.names = TRUE),
    # Store the path to each csv in a "path" column
    id = "path")
  
  # Now tidy up the dataset before returning it to the main workflow
  cleaned_data <- all_data %>%
    mutate(
      # Create a new column with a shortened version of the path
      site_code = gsub(pattern = full_folder_path,
                     replacement = "",
                     x = path),
      # Extract just the folder name from this shortened version of the path
      site_code = str_extract(string = site_code,
      # Remove everything after the first bit of upper case text
        pattern = "[A-Z]+"),
      
      DT =  as.POSIXct(datetime, format = "%m/%d/%Y %H:%M", tz = "MST")
    )
}

## All sites combined into 1 dataframe
# change "" to the file path were all_sites is located

stage_df <- read_and_combine(folder_path = "~/Repositories/cameron_peak_fire_study/data/all_sites_caprod_data",
                                    full_folder_path = "C:/Users/Sam Struthers/OneDrive - Colostate/Documents/Repositories/cameron_peak_fire_study/data/all_sites_caprod_data/")%>%
  select(DT, site_code, wtrhgt__5)%>%
  mutate(water_height_cm = wtrhgt__5 /10, 
        time =  format(DT, format = "%H:%M"), 
        site_code = str_extract(string = site_code, 
                                      pattern = "[A-Z]+"))%>%
  dplyr ::rename(water_height_mm = wtrhgt__5)



#Pull in field measured values

manual_measurements <- read.csv("data/manual_values_dec_2022.csv")%>%
      mutate(first_measurement_DT = as.POSIXct(first_measurement_DT, format = "%m/%d/%Y %H:%M", tz = "MST"), 
             last_measurement_DT = as.POSIXct(last_measurement_DT, format = "%m/%d/%Y %H:%M", tz = "MST"))

```

## Visual

-   check for missing data, weird end dates, remove any 0s near

```{r}
master_visual <- stage_df%>%
  ggplot(aes(x= DT, y = water_height_cm, color = site_code))+
  geom_line()
plot(master_visual)

indv_visual <- filter(stage_df, site_code == "SAWM")%>%
  ggplot(aes(x= DT, y = water_height_cm, color = site_code))+
  geom_line()
plot(indv_visual)
```

## Export Masterfile

```{r}
write_csv(stage_df, "data/all_sites_combined_UNCLEAN.csv")
```

# Field data adjustments:

In broad strokes, these functions/loops will take the sensor data pulled in above and manual values measured in the field to attempt to correct sensor data to match up field data. Workflow overview:

1.  Subset sensor df by a first and second field measurement.
2.  Step sensor up or down based on the difference between first field measurement and recorded values.
3.  Create a time factor for each subset of data which will allow for the driftR package to correct for sensor drift
4.  Use driftR functions to match up end of dataset with second field measurement.

##Functions to subset and adjust stage values

```{r}
##Function to adjust sensor stage values to match start measurement  
  
  
  adjust_stage <- function(site_name,start_date, end_date, manual_start, manual_end){
    stage_df_new<- stage_df%>%
     dplyr::filter(stage_df$site_code %in% site_name)%>%
     dplyr::filter(between(DT, start_date, end_date))%>%
      mutate(adjustment = manual_start - mean(head(water_height_cm, n = 4), na.rm = TRUE) , 
             adjusted_stage = water_height_cm + adjustment)
    
    return(stage_df_new)
    
   
  }
  
##Function to give dataset a time correction factor for each subset
    dr_factor_subset <- function(site_name, start_date, end_date, manual_start, manual_end){
      
    factor_df <-stepped_df%>%
     dplyr::filter(stepped_df$site_code %in% site_name)%>%
     dplyr::filter(between(DT, start_date, end_date))%>%
      mutate(time =  format(DT, format = "%H:%M"), 
             date = as.Date(DT, format = "%Y-%m-%d"))%>%
      arrange(DT)%>%
      dr_factor(., corrFactor = corrFac, dateVar = date, timeVar = time, keepDateTime = TRUE )
    
    
      return(factor_df)
    }
    
##Function to correct drift over a subset by looking at measured values and the previous 4
## dataframe must already have corrFac added or it will not run
    
    dr_correctOne_subset <- function(site_name, start_date, end_date, manual_end, manual_start){
      
    corrected_df <-factor_df%>%
     dplyr::filter(factor_df$site_code %in% site_name)%>%
     dplyr::filter(between(DT, start_date, end_date))
    
    last_sensor_cm = mean(tail(corrected_df$adjusted_stage, n = 4),na.rm = TRUE)
    
     drift_corrected_df<- dr_correctOne(corrected_df, sourceVar = adjusted_stage, cleanVar = corrected_stage_cm ,calVal = last_sensor_cm , calStd = manual_end , factorVar = corrFac)
    
      return(drift_corrected_df)
  }
  


 



```

## Preforming the adjustments

Using the functions written above and pmap, we can map over the manual measurements df and preform the adjustments.

```{r}






#Fit column names to match argument names in functions

manual_measurements_for_func <- manual_measurements%>%
  select(site_name = site_code ,start_date = first_measurement_DT, end_date = last_measurement_DT, manual_start = first_measurement_cm, manual_end = last_measurement_cm)%>%
  na.omit()%>%
  as_tibble()





##PMAP FOR ADJUSTMENT FUNCTIONS!

#Steps df for each subset between site visits to match the first measurement (start measurement)
stepped_df <- pmap_dfr(manual_measurements_for_func, adjust_stage)

# creates a time factor for each subset using the driftR factor function
factor_df <- pmap_dfr(manual_measurements_for_func, dr_factor_subset)


#using factor and last 4 measurements from manual end ,corrects for drift so that manual end and the last measurement match up

corrected_df <- pmap_dfr(manual_measurements_for_func, dr_correctOne_subset)




  
```

## Graphing to check work

```{r}
  
 indv_corrected_df <- filter(corrected_df, site_code == "BEAV")
 indv_manual_measurements <- filter(manual_measurements_for_func, site_name == "BEAV")
 
  
 indv_corrected_graph <- ggplot()+
  geom_line(data = indv_corrected_df, aes( x= DT , y= water_height_cm), color= "red")+
  geom_line(data = indv_corrected_df, aes( x= DT , y= corrected_stage_cm), color= "black")+
  geom_line(data = indv_corrected_df, aes( x= DT , y= adjusted_stage), color= "green", alpha = .5)+
  geom_point(data =  indv_manual_measurements, aes( x= start_date , y= manual_start), color = "blue")+
   theme_bw()
ggplotly(indv_corrected_graph)
plot(indv_corrected_graph)









```

## Computing daily averages

```{r}

daily_means <- corrected_df%>%
  select(water_height_cm, corrected_stage_cm, adjusted_stage, date, Site = site_code)%>%
  group_by(date, Site)%>%
  dplyr:: summarise(mean_sensor_stage_cm = mean(water_height_cm),
         mean_adjusted_stage_cm = mean(adjusted_stage),
         mean_corrected_stage_cm = mean(corrected_stage_cm))

#Match up with FCW and ISCO Samples 

site_labels <- c("FISH", "BEAV", "BENN", "SAWM", "PENN", "BLAK", "SHEP", "LBEA", "ROAR", "SAWM_ISCO", "FISH_ISCO", "LBEA_ISCO", 'BLAK_ISCO' )

cam_pk_chem <- read.csv("data/CamPkChem.csv")%>%
  filter(Era == "FCW" | Era == "ISCO")%>%
  filter(SiteLabel %in% site_labels)%>%
  mutate( date  = as.Date(Date, format = "%d-%b-%y"), 
          Year = year(date), 
          dayofyear = yday(date))%>%
  filter(Year >=2022)%>%
  filter(SampleType %in% c("NORM", "ISCO"))%>%
  select(date, SiteLabel, Era, EraSamp, dayofyear, SampleType)%>%
mutate(Site=ifelse(SiteLabel %in% c('FISH','FISH_ISCO'),"FISH",
                   ifelse(SiteLabel %in% c('LBEA','LBEA_ISCO'),"LBEA",
                   ifelse(SiteLabel %in% c('SAWM','SAWM_ISCO'),"SAWM",
                   ifelse(SiteLabel %in% c('BLAK','BLAK_ISCO'),"BLAK",SiteLabel )))))



sample_by_day <- cam_pk_chem%>%
  group_by(Site, date, SampleType)%>%
  dplyr::summarise( count = n())%>%
  ungroup()






daily_means_w_samples <- daily_means%>%
  left_join( select(sample_by_day, c(Site, date, count,SampleType )), by = c("Site", "date") )



sample_plot <- filter(daily_means_w_samples, Site == "FISH")%>%
  ggplot()+
  geom_line(aes(x= date, y = mean_sensor_stage_cm, group = Site, color = Site))+
  geom_line(aes(x=date, y = mean_corrected_stage_cm, color = "black" ))+
  geom_point(aes(x= date, y = count, color = SampleType ))

plot(sample_plot)

#Write daily averages file matched up with sample #

write_csv(select(daily_means_w_samples, c(date,mean_sensor_stage_cm,mean_corrected_stage_cm, mean_adjusted_stage_cm, Site, SampleType, count)), "data/stage_daily_mean_and_samples_cleaned.csv")

```

## Playing with rating curve package 'brdc'

```{r}
library("bdrc")

data(krokfors)

fish_fake_rating <- data.frame(W = c(1, 2, 5, 10, 20, 50, 75, 80,100), Q = c(1, 1.5, 2, 2.5, 2.7, 5, 10, 15, 20))

gplm.fit <- gplm(Q~W,krokfors)
gplm.fit_fish_fake <- gplm(Q~W,fish_fake_rating)

summary(gplm.fit_fish_fake)
summary(gplm.fit)

 plot(gplm.fit)
 plot(gplm.fit_fish_fake)
 
 h_grid <-data.frame(corrected_stage =  seq(8,9,by=0.01))%>%
   mutate(site_name = "TEST", 
          original_stage = seq(7,8,by=0.01))
 
rating_curve_fake_test <- predict(gplm.fit, newdata = h_grid$original_stage)



##Function to get rating curve for a specific site

get_rating_curve <- function(site){
  
  stage_discharge <- filter(manual_stage_Q, site_name == site)%>%
    select(W = manual_start, Q = discharge_m_s, site_name)
    
  rating_curve_fit <- gplm(Q~W, stage_discharge)
  
  Q_and_stage_df <- filter(corrected_df, site_name == site)%>%
    predict(rating_curve_fit, newdata = corrected_df$corrected_stage_cm)
  
  return(rating_curve_fit)
  
  
}
 

 
```
