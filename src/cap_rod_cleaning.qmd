---
title: "cap rod intake"
format: html
editor: visual
---

## Packages

```{r setup}
require("knitr")
opts_knit$set(root.dir = "~/Repositories/cameron_peak_fire_study/")
library(tidyverse)
library(lubridate)
library(ggplot2)
library(plotly)


```

## Correcting Functions

DriftR package is not up to date with Rstudio so I am sourcing the functions from a folder called driftR_functions in the src folder

```{r}
source("src/driftR_functions/dr_correctOne.R")
source("src/driftR_functions/dr_correctTwo.R")
source("src/driftR_functions/dr_drop.R")
source("src/driftR_functions/dr_factor.R")
source("src/driftR_functions/dr_read.R")
source("src/driftR_functions/dr_readSonde.R")
source("src/driftR_functions/dr_replace.R")
source("src/driftR_functions/notInOperator.R")
source("src/driftR_functions/parseTime.R")
source("src/driftR_functions/sondeCal.R")
source("src/driftR_functions/sondeClean.R")
source("src/driftR_functions/sondeRaw.R")

```

## Importing and Combining

RULES:

1.  All data must be in CSVs nested in the folders data/all_sites/

2.  All files must have naming convention: SITE\_#DATE#

3.  All files must have same number of columns and column names

4.  All files must have the same DateTime format (can be adjusted below)

5.  Run all_data first to get what the pattern for gsub will be

    1.  This is also a good way to test that none of your files are broken

6.  Use plots to double check everything

    1.  Use the plots to double check there are no zeros near site visits. If there are, remove them from the dataset manually! This will help the adjustments run correctly.

```{r}
#function testing to create one df with all caprod data
read_and_combine <- function(folder_path, full_folder_path){
  
  # This ensures that version 2.0+ of the readr package (tidyverse) is being
  # used, because it allows for reading multiple files
  readr::local_edition(2) 
  
  # Read in all data files from all
  all_data <- read_csv(
    # Provide a vector/list of filenames to read
    file = list.files(
      path = folder_path,
      # Search in all subfolders of `path` for csv files
      recursive = TRUE,
      # Use full filenames so R can find them
      full.names = TRUE),
    # Store the path to each csv in a "path" column
    id = "path")
  
  # Now tidy up the dataset before returning it to the main workflow
  cleaned_data <- all_data %>%
    mutate(
      # Create a new column with a shortened version of the path
      site_code = gsub(pattern = full_folder_path,
                     replacement = "",
                     x = path),
      # Extract just the folder name from this shortened version of the path
      site_code = str_extract(string = site_code,
      # Remove everything after the first bit of upper case text
        pattern = "[A-Z]+"),
      
      DT =  as.POSIXct(datetime, format = "%m/%d/%Y %H:%M", tz = "MST")
    )
}

## All sites combined into 1 dataframe
# change "" to the file path were all_sites is located

stage_df <- read_and_combine(folder_path = "~/Repositories/cameron_peak_fire_study/data/all_sites_caprod_data",
                                    full_folder_path = "C:/Users/Sam Struthers/OneDrive - Colostate/Documents/Repositories/cameron_peak_fire_study/data/all_sites_caprod_data/")%>%
  select(DT, site_code, wtrhgt__5)%>%
  mutate(water_height_cm = wtrhgt__5 /10, 
        time =  format(DT, format = "%H:%M"), 
        site_code = str_extract(string = site_code, 
                                      pattern = "[A-Z]+"))%>%
  dplyr ::rename(water_height_mm = wtrhgt__5)



#Pull in field measured values

manual_measurements <- read.csv("data/manual_values_dec_2022.csv")%>%
      mutate(first_measurement_DT = as.POSIXct(first_measurement_DT, format = "%m/%d/%Y %H:%M", tz = "MST"), 
             last_measurement_DT = as.POSIXct(last_measurement_DT, format = "%m/%d/%Y %H:%M", tz = "MST"))

```

## Visual

-   check for missing data, weird end dates, remove any 0s near

```{r}
master_visual <- stage_df%>%
  ggplot(aes(x= DT, y = water_height_cm, color = site_code))+
  geom_line()
plot(master_visual)

indv_visual <- filter(stage_df, site_code == "SAWM")%>%
  ggplot(aes(x= DT, y = water_height_cm, color = site_code))+
  geom_line()
plot(indv_visual)
```

## Export Masterfile

```{r}
write_csv(stage_df, "data/all_sites_combined_UNCLEAN.csv")
```

# Field data adjustments:

In broad strokes, these functions/loops will take the sensor data pulled in above and manual values measured in the field to attempt to correct sensor data to match up field data. Workflow overview:

1.  Subset sensor df by a first and second field measurement.
2.  Step sensor up or down based on the difference between first field measurement and recorded values.
3.  Create a time factor for each subset of data which will allow for the driftR package to correct for sensor drift
4.  Use driftR functions to match up end of dataset with second field measurement.

##Functions to subset and adjust stage values

```{r}
##Function to adjust sensor stage values to match start measurement  
  
  
  adjust_stage <- function(site_name,start_date, end_date, manual_start, manual_end){
    stage_df_new<- stage_df%>%
     dplyr::filter(stage_df$site_code %in% site_name)%>%
     dplyr::filter(between(DT, start_date, end_date))%>%
      mutate(adjustment = manual_start - mean(head(water_height_cm, n = 4), na.rm = TRUE) , 
             adjusted_stage = water_height_cm + adjustment)
    
    return(stage_df_new)
    
   
  }
  
##Function to give dataset a time correction factor for each subset
    dr_factor_subset <- function(site_name, start_date, end_date, manual_start, manual_end){
      
    factor_df <-stepped_df%>%
     dplyr::filter(stepped_df$site_code %in% site_name)%>%
     dplyr::filter(between(DT, start_date, end_date))%>%
      mutate(time =  format(DT, format = "%H:%M"), 
             date = as.Date(DT, format = "%Y-%m-%d"))%>%
      arrange(DT)%>%
      dr_factor(., corrFactor = corrFac, dateVar = date, timeVar = time, keepDateTime = TRUE )
    
    
      return(factor_df)
    }
    
##Function to correct drift over a subset by looking at measured values and the previous 4
## dataframe must already have corrFac added or it will not run
    
    dr_correctOne_subset <- function(site_name, start_date, end_date, manual_end, manual_start){
      
    corrected_df <-factor_df%>%
     dplyr::filter(factor_df$site_code %in% site_name)%>%
     dplyr::filter(between(DT, start_date, end_date))
    
    last_sensor_cm = mean(tail(corrected_df$adjusted_stage, n = 4),na.rm = TRUE)
    
     drift_corrected_df<- dr_correctOne(corrected_df, sourceVar = adjusted_stage, cleanVar = corrected_stage_cm ,calVal = last_sensor_cm , calStd = manual_end , factorVar = corrFac)
    
      return(drift_corrected_df)
  }
  


 



```

## Preforming the adjustments

Using the functions written above and pmap, we can map over the manual measurements df and preform the adjustments.

```{r}






#Fit column names to match argument names in functions

manual_measurements_for_func <- manual_measurements%>%
  select(site_name = site_code ,start_date = first_measurement_DT, end_date = last_measurement_DT, manual_start = first_measurement_cm, manual_end = last_measurement_cm)%>%
  na.omit()%>%
  as_tibble()





##PMAP FOR ADJUSTMENT FUNCTIONS!

#Steps df for each subset between site visits to match the first measurement (start measurement)
stepped_df <- pmap_dfr(manual_measurements_for_func, adjust_stage)

# creates a time factor for each subset using the driftR factor function
factor_df <- pmap_dfr(manual_measurements_for_func, dr_factor_subset)


#using factor and last 4 measurements from manual end ,corrects for drift so that manual end and the last measurement match up

corrected_df <- pmap_dfr(manual_measurements_for_func, dr_correctOne_subset)




  
```

## Graphing to check work

```{r}
  
 indv_corrected_df <- filter(corrected_df, site_code == "BEAV")
 indv_manual_measurements <- filter(manual_measurements_for_func, site_name == "BEAV")
 
  
 indv_corrected_graph <- ggplot()+
  geom_line(data = indv_corrected_df, aes( x= DT , y= water_height_cm), color= "red")+
  geom_line(data = indv_corrected_df, aes( x= DT , y= corrected_stage_cm), color= "black")+
  geom_line(data = indv_corrected_df, aes( x= DT , y= adjusted_stage), color= "green", alpha = .5)+
  geom_point(data =  indv_manual_measurements, aes( x= start_date , y= manual_start), color = "blue")+
   theme_bw()
ggplotly(indv_corrected_graph)
plot(indv_corrected_graph)






```
## Splitting sites where caprod was moved
LBEA and SHEP had to be moved midseason so they are now being split into LBEA1/2 and SHEP1/2

This section can be skipped if sensors did not move location thru the season

```{R}



# This dataframe contains the old names, new names and star/end dates for the name changes
# If there are more moves, this can be written in as a csv with the same  column names and read in using the hashed out code below

rename_df <- data.frame(old_name = c("LBEA", "LBEA", "SHEP","SHEP"), 
                       new_name = c("LBEA1", "LBEA2", "SHEP1", "SHEP2"), 
                       start_date = c("2022-05-01 12:00","2022-08-19 10:15","2022-05-01 12:00","2022-08-19 12:30"), 
                       end_date= c("2022-08-19 10:00","2022-11-01 12:00","2022-08-17 15:15","2022-11-01 12:00"))%>% mutate(start_date = as.POSIXct(start_date, format = "%Y-%m-%d %H:%M", tz = "MST"),
         end_date = as.POSIXct(end_date, format = "%Y-%m-%d %H:%M", tz = "MST"))

# rename_df <- read.csv("FILE_PATH_TO.CSV")%>% 
#mutate(start_date = as.POSIXct(start_date, format = "%Y-%m-%d %H:%M", tz = "MST"),
#          end_date = as.POSIXct(end_date, format = "%Y-%m-%d %H:%M", tz = "MST"))


change_site_names <- function(old_name, new_name, start_date, end_date){
  test_function <- corrected_df%>%
    dplyr:: filter(site_code == old_name & between(DT, start_date, end_date) )%>%
    mutate(site_code = new_name)
  
  return(test_function)
}

non_name_changed_df = corrected_df[!(corrected_df$site_code %in% unique(rename_df$old_name)),]

corrected_df<- pmap_dfr(rename_df, change_site_names)%>%
  rbind(non_name_changed_df)

#Check that all the new names are in df and old names removed
unique(corrected_df$site_code)
#If old names persist you may need to change the start/end dates so that all rows are included


```

##Add flag type column and associated title

Flags were determined visually and compared to field notes about the streams. 

```{r}


flag_values<- read.csv("data/flag_values_caprods_2022.csv")%>%
  mutate(start_dt = as.POSIXct(start_dt, format = "%m/%d/%Y %H:%M", tz = "MST"),
         end_dt = as.POSIXct(end_dt, format = "%m/%d/%Y %H:%M", tz = "MST"))

apply_flag <- function(site, FLAG, start_dt, end_dt, notes){
correct_and_flagged_df <- corrected_df%>%
  dplyr::filter(corrected_df$site_code %in% site)%>%
  dplyr::filter(between(DT,start_dt, end_dt))%>%
  mutate(flag_type = FLAG, 
         note = notes)

return(correct_and_flagged_df)
  
}

correct_and_flagged_df <- pmap_dfr(flag_values, apply_flag)

flag_colors <- c("PASS" = "green", "FAIL" = "red", "TREND" = "purple", "VARIATION"= "blue", "CHANNEL"= "orange") 

indv_site <- correct_and_flagged_df%>%
  filter(site_code == "SAWM")



indv_site_flags <- ggplot()+
  geom_point(data = indv_site, aes(x = DT, y= corrected_stage_cm, color = flag_type ))+
  scale_color_manual(values = flag_colors)+
  theme_bw()
                     
plot(indv_site_flags)                     
                    
                     
```
## Exporting 15min finalized data
```{r}
final_df<- correct_and_flagged_df%>%
  select(DT, site_code, 
         sensor_stage_mm = water_height_mm, 
         sensor_stage_cm = water_height_cm, 
         adjustment_cm = adjustment,
         adjusted_stage_cm = adjusted_stage,
         time_correction_value = corrFac, 
         corrected_stage_cm,
         flag_type, 
         notes = note,
         date, time)

write.csv(final_df, "data/corrected_15min_stage_CPF_2022.csv")

```

## Computing daily averages

```{r}

daily_means <- corrected_df%>%
  select(water_height_cm, corrected_stage_cm, adjusted_stage, date, site_code)%>%
  group_by(date, site_code)%>%
  dplyr:: summarise(mean_sensor_stage_cm = mean(water_height_cm),
         mean_adjusted_stage_cm = mean(adjusted_stage),
         mean_corrected_stage_cm = mean(corrected_stage_cm))%>%
  ungroup()%>%
   ##IGNORE UNLESS YOU NEED TO MATCH GRAB SAMPLES UP TO SITES
  mutate(site_label = ifelse(site_code %in% c("LBEA1", "LBEA2"), "LBEA",
                      ifelse(site_code %in% c("SHEP1", "SHEP2"), "SHEP",site_code)))
  ##HASH OUT EVERYTHING ABOVE IF NOT MATCHING SAMPLES AND 

#Match up with FCW and ISCO Samples 

site_labels <- c("FISH", "BEAV", "BENN", "SAWM", "PENN", "BLAK", "SHEP", "LBEA", "ROAR", "SAWM_ISCO", "FISH_ISCO", "LBEA_ISCO", 'BLAK_ISCO', "LBEAISCO", "BLAKISCO", "SAWMISCO", "FISHISCO" )

cam_pk_chem <- read.csv("data/CamPkChem.csv")%>%
  filter(Era == "FCW" | Era == "ISCO")%>%
  dplyr::rename(site_label = SiteLabel)%>%
  filter(site_label %in% site_labels)%>%
  mutate( date  = as.Date(Date, format = "%d-%b-%y"), 
          Year = year(date), 
          dayofyear = yday(date))%>%
  filter(Year >=2022)%>%
  filter(SampleType %in% c("NORM", "ISCO"))%>%
  select(date, site_label, Era, EraSamp, dayofyear, SampleType)%>%
mutate(site_label=ifelse(site_label %in% c('FISH','FISH_ISCO'),"FISH",
                   ifelse(site_label %in% c('LBEA','LBEA_ISCO', "LBEAISCO"),"LBEA",
                   ifelse(site_label %in% c('SAWM','SAWM_ISCO'),"SAWM",
                   ifelse(site_label %in% c('BLAK','BLAK_ISCO'),"BLAK",site_label )))))



sample_by_day <- cam_pk_chem%>%
  group_by(site_label, date, SampleType)%>%
  dplyr::summarise( count = n())%>%
  ungroup()

#create df of sites and codes for data flag but on a daily timescale

flag_daily_values<- flag_values%>%
  mutate(start_date = as.Date(start_dt), 
         end_date = as.Date(end_dt))%>%
  select(site, FLAG, start_date, end_date, notes)

#changed apply_flag to work on daily timestep rather than by DT

apply_flag_daily <- function(site, FLAG, start_date, end_date, notes){
correct_and_flagged_daily_df <- daily_means%>%
  dplyr::filter(between(date,start_date, end_date))%>%
  dplyr::filter(site_code == site)%>%
  mutate(flag_type = FLAG, 
         note = notes)

return(correct_and_flagged_daily_df)
  
}




correct_and_flagged_daily_df <- pmap_dfr(flag_daily_values, apply_flag_daily)



daily_means_w_samples <- correct_and_flagged_daily_df%>%
  left_join( select(sample_by_day, c(site_label, date, count,SampleType )), by = c("site_label", "date") )



sample_plot <- filter(daily_means_w_samples, site_code == "FISH")%>%
  ggplot()+
  geom_line(aes(x= date, y = mean_corrected_stage_cm, group = site_code, color = flag_type))+
  geom_line(aes(x= date, y = mean_sensor_stage_cm, group = site_code, color = flag_type))+
  scale_color_manual(values = flag_colors)+
  #geom_line(aes(x=date, y = mean_corrected_stage_cm, color = "red" ))+
  geom_point(aes(x= date, y = count, color = "grey" ))

plot(sample_plot)

```
##exporting corrected daily means data
```{r}

#Write daily averages file matched up with sample #

final_daily_means <- daily_means_w_samples%>%
  select(date,site_code, mean_sensor_stage_cm, mean_adjusted_stage_cm, mean_corrected_stage_cm,SampleType ,sampleCount = count, flag_type, note, site_label)

write_csv(final_daily_means, "data/stage_daily_mean_and_samples_cleaned.csv")



```


## Playing with rating curve package 'brdc'

```{r}
library("bdrc")

data(krokfors)

fish_fake_rating <- data.frame(W = c(1, 2, 5, 10, 20, 50, 75, 80,100), Q = c(1, 1.5, 2, 2.5, 2.7, 5, 10, 15, 20))

gplm.fit <- gplm(Q~W,krokfors)
gplm.fit_fish_fake <- gplm(Q~W,fish_fake_rating)

summary(gplm.fit_fish_fake)
summary(gplm.fit)

 plot(gplm.fit)
 plot(gplm.fit_fish_fake)
 
 h_grid <-data.frame(corrected_stage =  seq(8,9,by=0.01))%>%
   mutate(site_name = "TEST", 
          original_stage = seq(7,8,by=0.01))
 
rating_curve_fake_test <- predict(gplm.fit, newdata = h_grid$original_stage)



##Function to get rating curve for a specific site

get_rating_curve <- function(site){
  
  stage_discharge <- filter(manual_stage_Q, site_name == site)%>%
    select(W = manual_start, Q = discharge_m_s, site_name)
    
  rating_curve_fit <- gplm(Q~W, stage_discharge)
  
  Q_and_stage_df <- filter(corrected_df, site_name == site)%>%
    predict(rating_curve_fit, newdata = corrected_df$corrected_stage_cm)
  
  return(rating_curve_fit)
  
  
}
 

 
```
